{:title "\"The Genuine Sieve of Eratosthenes\" in Clojure"
 :layout :post
 :tags ["clojure" "papers"]}

After my previous two posts on primes ([first][primes1], [second][primes2]), a
few people have reached out through various channel to suggest alternative ways
to compute them. Eventually I ended up with [a Haskell paper][paper] in my
hands, and I'm apparently starting to make a habit out of translating those to
Clojure, so I thought I'd give this one a try too.

There is some overlap with my previous posts, but I'm going to write this one
to stand on its own as it doesn't feel right to assume every reader has read my
entire blog.

### Not the sieve

The paper starts with the observation that a lot of sources seem confused about
what the sieve _is_, and presents this code as a common example of something
people call a sieve when it's actually _not_:

```haskell
primes = sieve [2..]

sieve (p : xs) = p : sieve [x | x <- xs, x `mod` p > 0]
```

A direct translation to Clojure, for those unfamiliar with Haskell syntax,
would yield:

```clojure
(defn sieve
  [[p & xs]]
  (cons p
        (lazy-seq
          (sieve (for [x xs :when (pos? (mod x p))]
                   x)))))

(def primes (sieve (iterate inc 2)))
```

This doesn't work in Clojure because we keep wrapping the lazy seq in more and
more `filter` calls (implicitly through the `for` macro) and eventually those
blow the (admittedly limited) JVM stack. We could try to work around that by
increasing the stack size, but that's a very limited fix at best.

Haskell has a larger default stack, so that particular problem is less of an
issue, but the paper goes on to explain that this approach is still very slow.

In the rest of the paper, the author refers to this implementation as "the
unfaithful sieve"; I'll adopt the same name here to distinguish it from the
real sieves later on.

### What the sieve is

Eratosthenes did not have access to computers, so his technique was based on a
blackboard. It worked this way. First, write down all the integers you can fit
on your blackboard, starting at 2. Then, for each number \\(p\\) (starting at 2):

1. \\(p\\) is prime.
2. Erase all of the multiples of \\(p\\) on the blackboard. You can easily do
   that without any division (or multiplication) by just erasing every \\(p\\)th
   number. As a minor optimization, one can start at \\(p\^2\\) instead of at
   \\(p\\).
3. Find the first number after \\(p\\) that is still on the board. Set \\(p\\) to
   that and go back to 1.

As another optimization, if you know the maximum number you have written on the
board, \\(n\\), when you reach a situation in which \\(p \geq \sqrt{n}\\), you can
stop and consider all the remaining numbers to be prime.[^stop]

[^stop]: Incidentally, while I know the underlying math well enough that it's
obvious to me once I've seen it stated, I had not thought about it myself and
that optimization is not present in the code I presented in my previous two
posts about primes.

### Why the wrong one is bad

Is it obvious to you that this performs a lot less work than the unfaithful
sieve above? It wasn't immediately obvious to me, but the paper walks through a
simple example: what happens for 17. In the unfaithful sieve, the filter
function for 17 will get applied to every single integer that is not a
composite of the primes under 17 (2, 3, 5, 7, 11, 13). So, for example, we will
be computing `(mod 19 17)` and `(mod 23 17)`, which the "true" sieve flies
right over to 34 even if we don't implement the square optimization.

Looking at it from another angle, how many filters are we going to apply to 19?
Well, _all of them_, because 19 is prime: we will compute `(mod 19 p)` for `p`
in `[2 3 5 7 11 13 17]`. Obviously, this does not get better as primes grow and
the number of primes smaller than them grows accordingly. Whereas, in the real
sieve, we do nothing to 19 because it's prime.

For non-primes, i.e. the numbers that we do want to cross off, the unfaithful
sieve will stop at the first composite found, whereas the real sieve may cross
off the same number multiple times. For instance, 18 is divisible by both 2 and
3 and bigger than either \\(2\^2\\) or \\(3\^2\\), so it would get crossed twice.
But, even though the unfaithful sieve stops at the first composite it finds, it
still _tries_ a lot more of them. 18 specifically will be filtered out by our
first filter (as a multiple of 2), but if we look instead at 221 (\\(13 \times
17\\)), the unfaithful prime will compute `(mod 221 p)` for `p` in `[2 3 5 7 11
13]` (and stop at \\(13\\)). By contrast, the real sieve only crosses this number
twice.

So the unfaithful sieve does a lot more checking, and that's even before you
consider the difference between the actual check: a simple addition for the
real sieve, and a division for the unfaithful one.

[The paper][paper] goes on to calculate complexity classes; the details of the
calculations are not very relevant for this blog post, so I'll just write down
the results. The theoretical complexity of generating all the primes lower than
\\(n\\) is found to be:

\\[
\Theta (n \log \log n)
\\]

for the real sieve,

\\[
\Theta\left(\frac{n\^2}{(\log n)^2}\right)
\\]

for the unfaithful sieve, and

\\[
\Theta\left(\frac{n\sqrt{n}}{(\log n)^2}\right)
\\]

for a slightly better implementation of the same idea as the unfaithful sieve,
which would be written:

```haskell
primes = 2 : [x | x <- [3..], isprime x]
isprime x = all (\p -> x `mod` p > 0) (factorsToTry x)
  where factorsToTry x = takeWhile (\p -> p * p <= x) primes
```

or, in Clojure:

```clojure
(declare primes)

(defn prime?
  [x]
  (let [factors-to-try (fn [x] (take-while #(<= (* % %) x) primes))]
    (every? #(pos? (mod x %)) (factors-to-try x))))

(def primes
  (cons 2
        (for [x (iterate inc 3)
              :when (prime? x)]
          x)))
```

Local definitions and comprehensions are a little bit less common in Clojure
than in Haskell papers, so the following might be a bit more idiomatic:

```clojure
(declare primes)

(defn prime?
  [x]
  (->> primes
       (take-while #(<= (* % %) x))
       (every? #(pos? (mod x %)))))

(def primes
  (cons 2
        (->> (iterate inc 3)
             (filter prime?))))
```

### What about infinity?

The main drawback of the sieve approach at this point is that it is ill-suited
to the task of producing an infinite list, which both other approaches handle
very well. I have, in my previous posts, hacked around that by recomputing
the sieve on increasingly large limits (using [mutable arrays][primes1] or [bit
sets][primes2]), but the paper takes the better, harder route of actually
defining an inifinite sieve using a purely functional approach.

The obvious issue is step 2 in the sieve algorithm above: if we're dealing with
an infinite list, we can't erase all of the multiples of a prime. So, instead,
we store a list of all of the "next multiple". When we consider 3, for example,
that list only contains 4, so 3 is prime. When we consider 4, the list contains
4 and 9, so 4 is not prime. When we consider 10, the list contains 10 twice
(one from 2 and once from 5), 12 and 25. And so on. So we need to keep track
of, for each prime we have found so far, the "next" value it would cross off.
We also need to be able to efficiently ask of our list for the minimum value in
it. The algorithm becomes, roughly:

1. Start with \\(n = 3\\) and \\(p = [4]\\).
2. If \\(n < \min p\\), \\(n\\) is prime. Consider the next \\(n\\), and add \\(n^2\\)
   to the list. Go to 1.
3. If \\(n = \min p\\), then \\(n\\) is not prime. Replace each \\(n\\) in \\(p\\) with
   the next value for the prime that had produced it, then increment \\(n\\) and
   got to 1.

The tricky bit is step 3, where we need to know not only what the next smallest
non-prime is, but also which primes it can be composed of.

The paper proposes this first implementation, using a map of "next value" to
"list of primes that produced it" to keep track of provenance:

```haskell
sieve xs = sieve' xs Map.empty
  where
    sieve' [] table = []
    sieve' (x:xs) table = case Map.lookup x table of
      Nothing -> x : sieve' xs (Map.insert (x * x) [x] table)
      Just facts -> sieve' xs (foldl reinsert (Map.delete x table) facts)
      where
        reinsert table prime = Map.insertWith (++) (x + prime) [prime] table
```

A Clojure version of that would be:

```clojure
(defn sieve
  ([s] (sieve s {}))
  ([[x & xs] table]
   (if-let [factors (get table x)]
     (sieve xs (reduce (fn [t prime]
                         (update t (+ x prime) concat [prime]))
                       (dissoc table x)
                       factors))
     (cons x (lazy-seq (sieve xs (assoc table (* x x) [x])))))))
```

`primes` is still defined by calling `sieve` on a list of integers starting at
2.

### Data structures

Using a map here is not very efficient: we know we're only ever going to want
to look at the smallest key, so we do not need to pay the logarithmic cost of
random access in a tree-based map. Even for a trie-based map where access is
roughly constant (\\(\log_{32}n\\)) like Clojure's, the constant factors are
quite high.


[primes1]: /posts/2021-11-07-clj-primes
[primes2]: /posts/2021-11-28-clj-primes-2
[paper]: https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf
