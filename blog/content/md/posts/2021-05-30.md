{:title "What is functional programming?"
 :layout :post
 :tags ["paradigms"]}

Some people would argue that the dominant programming paradigm of the nineties
and aughts was "object-oriented". The same people would probably argue it still
is. There's a newcomer, though, that's been making a lot of headway over the
past twenty years or so: functional programming. Most people learn new things
by comparing them with things they already know, and it is thus fairly natural
for programmers well-versed in object-oriented techniques to ask how functional
code compares to object-oriented code.

This is, however, not a useful question. Functional code does not contrast with
object-oriented code, because they sit alongside different axes. They are
orthogonal to each other and, to a large extent, do not apply at the same
level.

In this post, I'll try to explain what functional programming is, and what it
should be contrasted to. One of the biggest hurdles for such a discussion is
that programming, as a discipline, does not have a widely-agreed-upon
jargon[^jargon] yet. For the purposes of this blog post, I assume
"object-oriented" is one of two things, which in my experience are at the heart
of the most common meanings for the term:

[^jargon]: Jargon is usually seen as a bad thing to the uninitiated, but the
development of jargon is actually a very important sign that a discipline is
maturing. Jargon is what lets experts in a field have useful discussions with
each other, where they can precisely refer to discipline-specific concepts and
ideas and reduce ambiguity and misunderstandings. Computer programming is still
a very young field of study; it has been pretty prolific in creating new words,
or appropriating existing words, but it has not yet reached the point where
those cords truly have agreed-upon, precise defintions across the discipline.
Even basic terms like "variable" or "type" can mean widely different things to
different people.

- An organization of code whereby data and the functions to manipulate that
  data are colocated under a common reference. This is the nuts-and-bolt
  "objects and classes" language-level set of features.
- An architectural technique whereby programs are divided into separate parts
  that communicate through message-passing. This can be done in any language.

## Models of computation

Functional programming, however, is a _model of computation_ based on Church's
"lambda calculus". It has pretty much nothing to do with either of the above
concepts. If a comparison needs to be made, it should be to _imperative
programming_, which is a model of computation based on Turing's "Turing
machine", and is inarguably the dominant programming paradigm of the last 200
hundred years.[^babbage]

[^babbage]: The field of computer programming is generally regarded as starting
in the 1950s, while the mathematical concept of a Turing machine was first
described by Turing in 1936. Still, Babbage's analytical engine was, at its
heart, a Turing machine, and thus imperative programming has been the dominant
paradigm since 1837.

So what's a model of computation? At the end of the 19th century, Cantor and
Russel broke mathematics.[^history] That lead mathematicians to question the
validity of their entire field, and thus to try and rigorously redefine
mathematics from the ground up in order to prove Cantor and Russel wrong. The
most important mathematical consequence from that effort is GÃ¶del's
incompleteness theorems, which proved that mathematics was, and would remain,
broken. Mathematicians have since then made their peace with that state of
affairs. That thread of enquiry also led a few mathematicians to ponder the
notion of _computation_: Cantor's argument made a pretty good case for the
existence of real numbers that exist and can be precisely defined, yet cannot
be computed.

[^history]: This is obviously a very simplified version of history, with some
  inaccuracies thrown in for dramatic effect.

This was a bit unsettling, and thus prompted a handful of mathematicians to
ponder the deeper meaning of _computation_: what does it mean, in a precise,
rigorous mathematical sense, to compute a number? How can we tell the
difference between "this number cannot be computed" and "we don't know how to
compute that number yet"? One of the first proposition for such a formal
definition of a _model for computation_ was Church's lambda calculus. Having a
model does not really answer the question, though. All you can say with a model
is "this number can be computed _within this model_", but what if the model
itself is incomplete?

A few years later, Turing proposed his idea of a Turing machine, but more
importantly he offered a proof that the set of numbers that can be computed
with his Turing machine is exactly the same as the set of numbers that can be
computed with Church's lambda calculus. Since then, a fairly large number of
alternative models of computation have been proposed, and so far none of them
has been proven to be able to compute anything the Turing machine cannot
compute. In other words, the Turing machine is exactly as powerful as the
lambda calculus, and no other model we know of is more powerful (i.e. "can
compute more things") than either.

Note that we do _not_ have a proof that there is not a more powerful model of
computation available. All we know is that so far we have not found one. The
idea there the Turing machine _is_ the most powerful model of computation (as
opposed to "the most powerful one _we have found so far_) is known as "the
Church-Turing hypothesis".

Why am I telling all of that? Mostly because I like to ramble, but also because
lambda calculus and Turing machines turn out to be really important for modern
computing, and understanding them at a superficial level can give you deep
insights into computer programming.

## Turing machines & imperative execution

Formally, a Turing machine is a 7-tuple \\(M=\\langle Q, \\Gamma, b, \\Sigma,
\\delta, q\_0, F\\rangle\\) where:

- \\(Q\\) is a finite set of _states_ (it has to be non-empty as it needs to
  contain at least \\(q\_0\\)).
- \\(\\Gamma\\) is an _alphabet_, i.e. a set of symbols (it has to be non-empty
  as it needs to contain at least \\(b\\)).
- \\(b\\in\\Gamma\\) is a special symbol of the alphabet, which can appear an
  infinite number of times on the _tape_ (see below for what the tape is); all
  other symbols in \\(\\Gamma\\) can only appear on the tape a finite number of
  times at any point in time.
- \\(\\Sigma \\subseteq (\\Gamma \\setminus \\{b\\})\\) is the set of symbols
  that are allowed to appear on the _initial tape contents_.
- \\(q\_0\\in Q\\) is the initial state of the machine.
- \\(F\\subseteq Q\\) is the set of final states.
- \\(\\delta: (Q\\setminus F)\\times\\Gamma\\to
  Q\\times\\Gamma\\times\\{L,R\\}\\) is a (partial) function called the
  _transition function_.

This may or may not look like a jumble of meaningless symbols, and quite
frankly understanding the precise meaning of each of those is not very
important to my point here. (I do enjoy looking at typeset mathematics,
though). These are just the components; a formal definition would also need to
proceed with exactly how they are used (i.e. what their semantics are), but I
prefer to drop the formality level a few notches at this point. For our
purposes, it is enough to know that a formal definition exists.

Informally, one can visualize a Turing machine as having three components:

- A _ribbon_, or _tape_, with things written on it. The ribbon has a starting
  point (generally assumed to be on the left), but it is infinite on the other
  side (generally the right side). It is one-dimensional, and is composed of
  cells, with exactly one symbol per cell. A given Turing machine defines the
  alphabet that can be used on the ribbon, \\(\\Gamma\\), but not the contents
  of the ribbon. This make sense if you think of the definition of the Turing
  machine as the formula for a function, and the (initial) content of the
  ribbon as the input to that function. When the machine "starts", the ribbon
  will contain a description of the input using symbols of \\(\\Sigma\\),
  followed by an infinite number of cells containing \\(b\\).
- A _head_, which is pointing to a given cell on the ribbon. It starts at 0
  (the "first" cell on the finite side of the ribbon).
- A "processor", which can direct the head to move either one cell to the right
  or one cell to the left, after having written a new symbol on the ribbon at
  its current position.

The operation of the processor is the core of the Turing machine, and is mostly
described by the \\(\\delta\\) function. The Turing machine must be thought as
performing a number of _steps_, where each step consists of the following
events:

1. Check the current state. If the current state is in \\(F\\), the machine has
   finished running. Stop here. Exactly what it means for the machine to have
   finished depends on how the machine was defined, but in most cases it means
   the output of the function is a combination of the current state of the
   machine and what is currently written on the ribbon.
2. Read the symbol on the ribbon, at the current position of the head. Apply
   the \\(\\delta\\) function, using as arguments the symbol that was just read
   and the current state of the machine. The result gives us a new state for
   the processor, a new symbol that we write on the ribbon (possibly the same
   one we just read), and a direction to move the head into.
3. GOTO 1.

Note that, while there is no "internal structure" to the states of the
processor in a Turing machine, this is not a limitation as far as
mathematicians care: any internal structure can just be expanded to a set of
individual states by enumerating all of the possible values of all internal
variables. Additionally, while the formal definition only allows the head to
move one cell at a time, it is trivial to implement larger moves by simply
adding intermediate state that do nothing but rewrite what they just read and
keep moving.

As a concrete example, a Turing machine that does 2-bit addition could be
defined by:

- \\(Q := \\{B, B\_0, B\_1, B\_2, B\_3, H\_0, H\_1, H\_2, H\_3, H\\}\\)
- \\(\\Gamma := \\{0, 1, 2, 3, x\\}\\)
- \\(b := x\\)
- \\(\\Sigma := \\{0, 1, 2, 3\\}\\)
- \\(q\_0 := BB\\)
- \\(F := \\{H\\}\\)
- \\(\\delta\\) is defined by the following table:

<br />

| State | Symbol | to  | New state | Written symbol | Move |
|:-----:|:------:|:---:|:---------:|:--------------:|:----:|
| \\(B\\) | \\(0\\)| \\(\\to\\) | \\(B\_0\\) | \\(x\\)| \\(R\\)|
| \\(B\\) | \\(1\\)| \\(\\to\\) | \\(B\_1\\) | \\(x\\)| \\(R\\)|
| \\(B\\) | \\(2\\)| \\(\\to\\) | \\(B\_2\\) | \\(x\\)| \\(R\\)|
| \\(B\\) | \\(3\\)| \\(\\to\\) | \\(B\_3\\) | \\(x\\)| \\(R\\)|
| \\(B\_0\\) | \\(0\\)| \\(\\to\\) | \\(H\_0\\) | \\(x\\)| \\(L\\)|
| \\(B\_0\\) | \\(1\\)| \\(\\to\\) | \\(H\_1\\) | \\(x\\)| \\(L\\)|
| \\(B\_0\\) | \\(2\\)| \\(\\to\\) | \\(H\_2\\) | \\(x\\)| \\(L\\)|
| \\(B\_0\\) | \\(3\\)| \\(\\to\\) | \\(H\_3\\) | \\(x\\)| \\(L\\)|
| \\(B\_1\\) | \\(0\\)| \\(\\to\\) | \\(H\_1\\) | \\(x\\)| \\(L\\)|
| \\(B\_1\\) | \\(1\\)| \\(\\to\\) | \\(H\_2\\) | \\(x\\)| \\(L\\)|
| \\(B\_1\\) | \\(2\\)| \\(\\to\\) | \\(H\_3\\) | \\(x\\)| \\(L\\)|
| \\(B\_1\\) | \\(3\\)| \\(\\to\\) | \\(H\_0\\) | \\(x\\)| \\(L\\)|
| \\(B\_2\\) | \\(0\\)| \\(\\to\\) | \\(H\_2\\) | \\(x\\)| \\(L\\)|
| \\(B\_2\\) | \\(1\\)| \\(\\to\\) | \\(H\_3\\) | \\(x\\)| \\(L\\)|
| \\(B\_2\\) | \\(2\\)| \\(\\to\\) | \\(H\_0\\) | \\(x\\)| \\(L\\)|
| \\(B\_2\\) | \\(3\\)| \\(\\to\\) | \\(H\_1\\) | \\(x\\)| \\(L\\)|
| \\(B\_3\\) | \\(0\\)| \\(\\to\\) | \\(H\_3\\) | \\(x\\)| \\(L\\)|
| \\(B\_3\\) | \\(1\\)| \\(\\to\\) | \\(H\_0\\) | \\(x\\)| \\(L\\)|
| \\(B\_3\\) | \\(2\\)| \\(\\to\\) | \\(H\_1\\) | \\(x\\)| \\(L\\)|
| \\(B\_3\\) | \\(3\\)| \\(\\to\\) | \\(H\_2\\) | \\(x\\)| \\(L\\)|
| \\(H\_0\\) | \\(x\\)| \\(\\to\\) | \\(H\\) | \\(0\\)| \\(R\\)|
| \\(H\_1\\) | \\(x\\)| \\(\\to\\) | \\(H\\) | \\(1\\)| \\(R\\)|
| \\(H\_2\\) | \\(x\\)| \\(\\to\\) | \\(H\\) | \\(2\\)| \\(R\\)|
| \\(H\_3\\) | \\(x\\)| \\(\\to\\) | \\(H\\) | \\(3\\)| \\(R\\)|

<br />

There are missing entries in that table. For example, if the machine ever reads
a \\(2\\) symbol on the ribbon while in the \\(H\_0\\) state, the \\(\\delta\\)
function is undefined. In such cases, we say that the computation failed.

Also note how the set of states really represents two internal variables: where
we are in the computation (not read anything yet, need to read one more
variable, need to write result, done) and an integer accumulator to carry out
the actual addition.

In order to compute 2 + 1 using this Turing machine, we would set up the ribbon
with the following symbols:

\\[2, 1, x, x, \\dots\\]

The entire state of the computation could be written:

\\[B:\\{[2], 1, x, x, \\dots\\}\\]

where \\(B\\) represents the current state of the machine and \\([]\\)
represents the current position of the head. Using this notation, the
computation would proceed as follows:

1. \\(B:\\{[2], 1, x, x, \\dots\\}\\): \\(B\\) is not in \\(F\\), so we proceed.
   \\(\\delta(B, 2)\\to\\{B\_2, x, R\\}\\), so the next state is:
2. \\(B\_2:\\{x, [1], x, x, \\dots\\}\\): \\(B\_2\\) is not in \\(F\\), so we proceed.
   \\(\\delta(B\_2, 1)\\to\\{H\_3, x, L\\}\\), so the next state is:
3. \\(H\_3:\\{[x], x, x, x, \\dots\\}\\): \\(H\_3\\) is not in \\(F\\), so we proceed.
   \\(\\delta(H\_3, x)\\to\\{H, 3, R\\}\\), so the next state is:
4. \\(H:\\{3,[x],x,x,\\dots\\}\\): \\(H\\) _is_ in \\(F\\), so we stop.

We can now read the ribbon, and we have computed \\(2 + 1 = 3\\).

The model of a Turing machine is powerful enough to define the model of a
Turing machine. That is, it is possible to define a specific Turing machine,
generally called "the unversal Turing machine" (UTM). More precisely:

- The alphabet of the UTM is righ enough that one can describe any other Turing
  machine using it.
- That definition would include the specification of the input alphabet of the
  "simulated" Turing machine.
- A given run of the UTM would correspond to a given run of the simulated
  machine _on a specific input for that machine_; therefore the initial ribbon
  of the UTM can be described as containing two parts: first, a description of
  the simulated turing machine, and second, the input for the simulated Turing
  machine.

The UTM can simulate itself, etc., though it obviously gets really tedious
really fast.

Why is any of this relevant? If you squint a little bit, a Turing machine is a
really good description of modern CPUs. The CPU reads from memory (which is
indexed 0 to many), the CPU has internal state (registers), and the CPU makes
its computations one step at a time by reading an instruction from memory,
changing its internal state, and moving on to another location in memory.

It is also a very good model to represent the state of a computer programmer
trying to understand imperative code. Imagine the following C code:

```c
/* 1 */  int x = 1;
/* 2 */  int i = 1;
         do {
/* 3 */    x = x * i;
/* 4 */    i = i + 1;
/* 5 */  } while (i <= 4);
/* 6 */  printf("%d\n", x);
```

In order to predict what will be printed, a programmer would read the code one
line at a time, updating their internal state of believe for what the value of
all variables is at each step. Calling \\(H\\) the current line (for _head_),
here is how such an evaluation would go:

- \\(\\{x = ?, i = ?, H = 1\\}\\): `x = 1`.
- \\(\\{x = 1, i = ?, H = 2\\}\\): `i = 1`.
- \\(\\{x = 1, i = 1, H = 3\\}\\): `x = x * i` we need to set \\(x\\) to \\(1\\).
- \\(\\{x = 1, i = 1, H = 4\\}\\): `i = i + 1` we need to set \\(i\\) to \\(2\\).
- \\(\\{x = 1, i = 2, H = 5\\}\\): `i <= 4` is true so we set \\(H\\) to \\(3\\).
- \\(\\{x = 1, i = 2, H = 3\\}\\): `x = x * i` we need to set \\(x\\) to \\(2\\).
- \\(\\{x = 2, i = 2, H = 4\\}\\): `i = i + 1` we need to set \\(i\\) to \\(3\\).
- \\(\\{x = 2, i = 3, H = 5\\}\\): `i <= 4` is true so we set \\(H\\) to \\(3\\).
- \\(\\{x = 2, i = 3, H = 3\\}\\): `x = x * i` we need to set \\(x\\) to \\(6\\).
- \\(\\{x = 6, i = 3, H = 4\\}\\): `i = i + 1` we need to set \\(i\\) to \\(4\\).
- \\(\\{x = 6, i = 4, H = 5\\}\\): `i <= 4` is true so we set \\(H\\) to \\(3\\).
- \\(\\{x = 6, i = 4, H = 3\\}\\): `x = x * i` we need to set \\(x\\) to \\(24\\).
- \\(\\{x = 24, i = 4, H = 4\\}\\): `i = i + 1` we need to set \\(i\\) to \\(5\\).
- \\(\\{x = 24, i = 5, H = 5\\}\\): `i <= 4` is **false** so we set \\(H\\) to \\(6\\).

And the program reaches its end.

> Like a Turing machine, _imperative code_ is _executed_ by reading
> instructions that tell:
> 1. How to update the internal state of a processor, and
> 2. Where to look for the next instruction.

## Lambda calculus & functional evaluation

Lambda calculus is formally defined as performing reduction operations on
lambda terms, where lambda terms are built using these:

- A sequence of roman-alphabet letters (usually just one letter) is a
  _variable_.
- An expression of the form \\((\\lambda x. M)\\), where \\(x\\) is a variable
  and \\(M\\) is a term, is a function definition, also called an
  _abstraction_.
- An expression of the form \\((M N)\\) where both \\(N\\) and \\(M\\) are
  terms is the _application_ of the function \\(M\\) evaluates to to the
  argument \\(N\\).

And "reduction" is defined by the two following rules:

- \\(\\lambda x. M[x]\\) can be rewritten to \\(\\lambda y. M[y]\\). This is
  called \\(\alpha\\)-converstion. The notation \\(M[x]\\) here means the
  variable \\(x\\) appears as a _free variable_ in the term \\(M\\). This is
  mostly there to reduce confusion for humans who might be confused by name
  collisions.
- \\(((\\lambda x. M) E) \\to (M[x := E])\\), called \\(\\beta\\)-reduction,
  consists in replacing all of the _bound_ instances of \\(x\\) in \\(M\\) with
  the term \\(E\\).

As a concrete example, the term:

\\[(\\lambda x. x)(\\lambda x. x)z\\]

can be rewritten to just \\(z\\) through the following sequence of steps:

- \\((\\lambda x. x)(\\lambda x.x) z\\): \\(\\alpha\\)-reduction to rename
  \\(x\\) to \\(y\\) in the second term, mostly for clarity.
- \\((\\lambda x. x)(\\lambda y.y) z\\):\\(\\beta\\)-reduction of the two right-most terms.
- \\((\\lambda x. x) z\\): \\(\\beta\\)-reduction.
- \\(z\\).

Just like the Turing machine is an appropriate model for how a programmer would
approach simulating imperative code, lambda calculus is an appropriate model
for how a programmer should approach simulating functional code. Using Scheme
as a notation, the above example could be written:

```scheme
((lambda (x) x) ((lambda (x) x) z)) ;; alpha-reduction
((lambda (x) x) ((lambda (y) y) z)) ;; beta-reduction
((lambda (x) x) z) ;; beta-reduction
z
```

Similarly, the following expression:

```scheme
(factorial 4)
```

given the definition

```scheme
(define factorial
  (lambda (n)
    (if (= 1 n)
      1
      (* n (factorial (- n 1))))))
```

would be evaluated using the following steps:

- `(factorial 4)`: replacing `factorial` with its definition.
- `((lambda (n) (if (= 1 n) 1 (* n (factorial (- n 1))))) 4)`: \\(\\beta\\) reduction.
- `(if (= 1 4) 1 (* 4 (factorial (- 4 1))))`: evaluating `(= 1 4)`.
- `(if false 1 (* 4 (factorial (- 4 1))))`: applying `if`.
- `(* 4 (factorial (- 4 1)))`: evaluating `(- 4 1)`.
- `(* 4 (factorial 3))`: replacing `factorial` with its definition.
- `(* 4 ((lambda (n) (if (= 1 n) 1 (* n (factorial (- n 1))))) 3))`: \\(\\beta\\) reduction.
- `(* 4 (if (= 1 3) 1 (* 3 (factorial (- 3 1)))))`: evaluating `(= 1 3)`.
- `(* 4 (if false 1 (* 3 (factorial (- 3 1)))))`: applying `if`.
- `(* 4 (* 3 (factorial (- 3 1))))`: evaluating `(- 3 1)`.
- `(* 4 (* 3 (factorial 2)))`: replacing `factorial` with its definition.
- `(* 4 (* 3 ((lambda (n) (if (= 1 n) 1 (* n (factorial (- n 1))))) 2)))`: \\(\\beta\\) reduction.
- `(* 4 (* 3 (if (= 1 2) 1 (* 2 (factorial (- 2 1))))))`: evaluating `(= 1 2)`.
- `(* 4 (* 3 (if false 1 (* 2 (factorial (- 2 1))))))`: applying `if`.
- `(* 4 (* 3 (* 2 (factorial (- 2 1)))))`: evaluating `(- 2 1)`.
- `(* 4 (* 3 (* 2 (factorial 1))))`: replacing `factorial` with its definition.
- `(* 4 (* 3 (* 2 ((lambda (n) (if (= 1 n) 1 (* n (factorial (- n 1))))) 1))))`: \\(\\beta\\) reduction.
- `(* 4 (* 3 (* 2 (if (= 1 1) 1 (* 1 (factorial (- 1 1)))))))`: evaluating `(= 1 1)`.
- `(* 4 (* 3 (* 2 (if true 1 (* 1 (factorial (- 1 1)))))))`: applying `if`.
- `(* 4 (* 3 (* 2 1)))`: evaluating `(* 2 1)`.
- `(* 4 (* 3 2))`: evaluating `(* 3 2)`.
- `(* 4 6)`: evaluating `(* 4 6)`.
- `24`.

Ignoring the syntactic difference, those last three steps are exactly what one
would do when computing basic arithmetic: each line is a completely
self-contained description of the current state of the computation, and
progress is made by choosing a subset of the computation and evaluating it. For
example;

- \\(1 + 2 + 4 * 7 - 13\\): evaluating \\(1 + 2\\).
- \\(3 + 4 * 7 - 13\\): evaluating \\(4 * 7\\).
- \\(3 + 28 - 13\\): evaluating \\(28 - 13\\).
- \\(3 + 15\\): evaluating \\(3 + 15\\).
- \\(18\\).

> Like lambda calculuse, _fucntional code_ is _evaluated_ by applying these
> three reduction technques on an expression:
> - Replacing a name by its definition.
> - \\(\\beta\\)-reduction of a lambda term.
> - Evaluation of "leaf" operators.
> Note the absence of state to carry within the processor.

## Why these two models?

There are many other models of computation out there, but they are not (yet?)
very interesting to the practicing programmer.

The Turing machine is important because of its mechanical nature: it can be
built. In fact, the first design for building such a machine predates their
formal mathematical definition by almost exactly a hundred year.

Lambda calculus is important because it treats computation just like any other
mathematical expression, which means that one can _reason_ about code using all
the techniques and tools developed by mathematicians over the past three
thousand years, as well as reuse all of the training and intuition one has
built while studying other branches of mathematics.

## Equivalence proof

The formal proof of equivalence is way outside the scope of this blog. The
general principle behind the proof, however, has profound implications for the
working programming. In a nutshell, in order to prove that two models of
computation are equivalent, you need to prove that each is "as powerful" as the
other. In order to prove that model \\(A\\) is as powerful as model \\(B\\),
you need to prove that you can express any computation model \\(B\\) can
express in model \\(A\\).

In the specific case of \\(B\\) being Turing machines, it is sufficient to show
that you can implement a universal Turing machine in model \\(A\\). Lambda
calculus similarly has a notion of a universal function.

In a less formal sense, as programmers we can think of models of computation as
programming languages. In this metaphor, two models of computation are
equivalent if you can write an interpreter for the other one in each of them.

The practical implication is that most imperative languages have at least a
subset of functional code in them. It usually follows the distinction between
"expressions" (functional) and "instructions" (imperative).

## Effects

One property the Turing machine has is a well-defined notion of _time_: the
head must move through the cells one at a time. This makes the Turing machine
an easier model when it comes to comparing the performance of two different
ways to compute the same function.

This also makes the Turing machine better at
